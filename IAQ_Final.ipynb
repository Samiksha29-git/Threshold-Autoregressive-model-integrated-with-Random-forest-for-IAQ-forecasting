{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First, uninstall the existing PyTorch installation\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "print(\"PyTorch imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "WtVTzvDvnevk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import seaborn as sns\n",
        "import random\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "5b_QFOEapV5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/CPCB.csv\", parse_dates=['Datetime'], dayfirst=True)\n",
        "# data = pd.read_csv('/content/drive/MyDrive/CPCB.csv', parse_dates=['Datetime'])"
      ],
      "metadata": {
        "id": "1j8RRv1dpYOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "utNELSTXpaaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "eChA_IE_pdrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "ZI0s2Ao2pgDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "XDXF6_YvpiTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n",
        "\n",
        "def replace_outliers_with_median(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    median = df[column].median()\n",
        "\n",
        "    # Replace outliers with median\n",
        "    df[column] = df[column].apply(lambda x: median if x < lower_bound or x > upper_bound else x)\n",
        "    return df\n",
        "\n",
        "# Apply to the column\n",
        "df_cleaned = replace_outliers_with_median(df, 'PM2.5')\n",
        "print(df_cleaned)"
      ],
      "metadata": {
        "id": "dp3vmAXFpkqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(df_cleaned)\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicate_mask = df.duplicated()\n",
        "\n",
        "# Print duplicate rows\n",
        "print(\"Duplicate rows:\\n\", df[duplicate_mask])\n",
        "\n",
        "df_no_duplicates = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "9oa1DtUPpnNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stationarity check using ADF"
      ],
      "metadata": {
        "id": "08CRN0ORpLeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_stationarity_adf(ata):\n",
        "    results = {}\n",
        "    for col in df.select_dtypes(include='number').columns:\n",
        "        series = data[col].dropna()\n",
        "        adf_result = adfuller(series)\n",
        "\n",
        "        results[col] = {\n",
        "            'ADF Statistic': adf_result[0],\n",
        "            'p-value': adf_result[1],\n",
        "            'Stationary': adf_result[1] < 0.05  # True if p < 0.05\n",
        "        }\n",
        "\n",
        "    return pd.DataFrame(results).T\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "adf_results = check_stationarity_adf(df)\n",
        "print(adf_results)"
      ],
      "metadata": {
        "id": "gfMVItPWpNj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation coefficient check"
      ],
      "metadata": {
        "id": "KuDFrL3upHON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute correlation matrix\n",
        "corr_matrix = df.select_dtypes(include=np.number).corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y2svD7GBpI8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Granger Test"
      ],
      "metadata": {
        "id": "wrIAWK8zpE-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose 'data' is your scaled or differenced dataframe with columns: ['PM2.5', 'SO2', 'CO', 'PM10']\n",
        "df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# ðŸ”¹ 2. Granger Causality Tests\n",
        "\n",
        "# Choose the maximum lag based on your VAR model selection (example: 5)\n",
        "maxlag = 5\n",
        "\n",
        "print(\"\\nGranger Causality Tests:\")\n",
        "for target in df_numeric.columns:\n",
        "    print(f\"\\nTarget Variable: {target}\")\n",
        "    for predictor in df_numeric.columns:\n",
        "        if target != predictor:\n",
        "            print(f\"  Testing if {predictor} causes {target}\")\n",
        "            test_result = grangercausalitytests(df_numeric[[target, predictor]], maxlag=maxlag, verbose=False)\n",
        "            # Print p-values for each lag\n",
        "            for lag in range(1, maxlag + 1):\n",
        "                p_value = test_result[lag][0]['ssr_ftest'][1]\n",
        "                print(f\"    Lag {lag}: p-value = {p_value:.4f}\")"
      ],
      "metadata": {
        "id": "rqnqM7z9o_D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PM2.5\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Load and prepare data\n",
        "df = pd.read_csv('/content/drive/MyDrive/CPCB.csv', parse_dates=['Datetime'])\n",
        "df.set_index('Datetime', inplace=True)\n",
        "\n",
        "features = ['PM2.5', 'SO2', 'CO', 'PM10', 'Temp', 'RH']\n",
        "df['PM2.5_roll3'] = df['PM2.5'].rolling(9).mean()\n",
        "df['Temp_roll3'] = df['Temp'].rolling(9).mean()\n",
        "df = df.dropna()\n",
        "\n",
        "df = df[features].dropna()\n",
        "\n",
        "# Split data\n",
        "n = len(df)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.85)\n",
        "\n",
        "train = df.iloc[:train_end]\n",
        "val = df.iloc[train_end:val_end]\n",
        "test = df.iloc[val_end:]\n",
        "\n",
        "# Lag function\n",
        "def create_lagged_data(df, lags=1):\n",
        "    df_lagged = df.copy()\n",
        "    for col in df.columns:\n",
        "        for lag in range(1, lags + 1):\n",
        "            df_lagged[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
        "    return df_lagged.dropna()\n",
        "\n",
        "# TAR model with polynomial features\n",
        "def fit_tar_model_poly(df_lagged, threshold):\n",
        "    poly = PolynomialFeatures(degree=1, include_bias=False)\n",
        "\n",
        "    regime_low = df_lagged[df_lagged['PM2.5_lag1'] <= threshold]\n",
        "    regime_high = df_lagged[df_lagged['PM2.5_lag1'] > threshold]\n",
        "\n",
        "    X_low = regime_low.drop(columns='PM2.5')\n",
        "    y_low = regime_low['PM2.5']\n",
        "    X_high = regime_high.drop(columns='PM2.5')\n",
        "    y_high = regime_high['PM2.5']\n",
        "\n",
        "    X_low_poly = poly.fit_transform(X_low)\n",
        "    X_high_poly = poly.fit_transform(X_high)\n",
        "\n",
        "    model_low = Ridge(alpha=0.1).fit(X_low_poly, y_low)\n",
        "    model_high = Ridge(alpha=0.1).fit(X_high_poly, y_high)\n",
        "\n",
        "\n",
        "    return model_low, model_high, threshold, poly\n",
        "\n",
        "# Prediction function\n",
        "def predict_tar_poly(df_lagged, model_low, model_high, threshold, poly):\n",
        "    X = df_lagged.drop(columns='PM2.5')\n",
        "    X_poly = poly.transform(X)\n",
        "    condition = df_lagged['PM2.5_lag1'] <= threshold\n",
        "    preds = np.where(condition,\n",
        "                     model_low.predict(X_poly),\n",
        "                     model_high.predict(X_poly))\n",
        "    return preds\n",
        "\n",
        "# Grid search over threshold\n",
        "df_lagged = create_lagged_data(df, lags=1)\n",
        "\n",
        "train_lagged = df_lagged.loc[train.index.intersection(df_lagged.index)]\n",
        "val_lagged = df_lagged.loc[val.index.intersection(df_lagged.index)]\n",
        "test_lagged = df_lagged.loc[test.index.intersection(df_lagged.index)]\n",
        "\n",
        "# Manually set best threshold and fit model\n",
        "best_thresh = 10.83\n",
        "model_low, model_high, _, poly = fit_tar_model_poly(train_lagged, best_thresh)\n",
        "best_models = (model_low, model_high, poly)\n",
        "\n",
        "# Optional: evaluate val RÂ² again\n",
        "yhat_val = predict_tar_poly(val_lagged, model_low, model_high, best_thresh, poly)\n",
        "best_r2 = r2_score(val_lagged['PM2.5'], yhat_val)\n",
        "print(f\"\\n Best threshold used: {best_thresh:.2f} with validation RÂ²: {best_r2:.4f}\")\n",
        "\n",
        "\n",
        "# Final model\n",
        "model_low, model_high, poly = best_models\n",
        "\n",
        "# Final predictions\n",
        "y_train = train_lagged['PM2.5']\n",
        "y_val = val_lagged['PM2.5']\n",
        "y_test = test_lagged['PM2.5']\n",
        "\n",
        "yhat_train = predict_tar_poly(train_lagged, model_low, model_high, best_thresh, poly)\n",
        "yhat_val = predict_tar_poly(val_lagged, model_low, model_high, best_thresh, poly)\n",
        "yhat_test = predict_tar_poly(test_lagged, model_low, model_high, best_thresh, poly)\n",
        "\n",
        "# Metrics\n",
        "def mase(y_true, y_pred, naive_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred)) / np.mean(np.abs(y_true[1:] - naive_pred[:-1]))\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    naive = y_true.shift(1).dropna()\n",
        "    y_true = y_true.iloc[1:]\n",
        "    y_pred = y_pred[1:]\n",
        "    return {\n",
        "        'R2': r2_score(y_true, y_pred),\n",
        "        'MAE': mean_absolute_error(y_true, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'MASE': mase(y_true, y_pred, naive)\n",
        "    }\n",
        "\n",
        "print(\"\\n Final Evaluation with Polynomial TAR:\")\n",
        "print(\"Train:\", evaluate(y_train, yhat_train))\n",
        "print(\"Val:  \", evaluate(y_val, yhat_val))\n",
        "print(\"Test: \", evaluate(y_test, yhat_test))\n",
        "\n",
        "# Plots\n",
        "def plot_actual_vs_pred(y_true, y_pred, title):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(y_true.index, y_true, label='Actual')\n",
        "    plt.plot(y_true.index, y_pred, label='Predicted')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('PM2.5')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_actual_vs_pred(y_train, yhat_train, 'Train: Actual vs Predicted')\n",
        "plot_actual_vs_pred(y_val, yhat_val, 'Validation: Actual vs Predicted')\n",
        "plot_actual_vs_pred(y_test, yhat_test, 'Test: Actual vs Predicted')\n",
        "\n",
        "# Residuals ACF and PACF\n",
        "residuals = y_test - yhat_test\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_acf(residuals, lags=30)\n",
        "plt.title(\"ACF of Test Residuals\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_pacf(residuals, lags=30)\n",
        "plt.title(\"PACF of Test Residuals\")\n",
        "plt.show()\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create lagged features for residual correction (train only on past data)\n",
        "def create_residual_features(df_lagged, residuals, lags=1):\n",
        "    df_feat = df_lagged.copy()\n",
        "    df_feat['residuals'] = residuals\n",
        "    for lag in range(1, lags + 1):\n",
        "        df_feat[f'residuals_lag{lag}'] = df_feat['residuals'].shift(lag)\n",
        "    df_feat = df_feat.dropna()\n",
        "    return df_feat\n",
        "\n",
        "# Step 1: Compute residuals for train/val/test\n",
        "residuals_train = y_train - yhat_train\n",
        "residuals_val = y_val - yhat_val\n",
        "residuals_test = y_test - yhat_test\n",
        "\n",
        "# Step 2: Add residuals to original lagged features\n",
        "train_feat = create_residual_features(train_lagged.copy(), residuals_train, lags=1)\n",
        "val_feat = create_residual_features(val_lagged.copy(), residuals_val, lags=1)\n",
        "test_feat = create_residual_features(test_lagged.copy(), residuals_test, lags=1)\n",
        "\n",
        "# Ensure alignment\n",
        "res_target_train = train_feat['residuals']\n",
        "res_target_val = val_feat['residuals']\n",
        "res_target_test = test_feat['residuals']\n",
        "\n",
        "# Drop target from features\n",
        "train_feat_X = train_feat.drop(columns=['residuals'])\n",
        "val_feat_X = val_feat.drop(columns=['residuals'])\n",
        "test_feat_X = test_feat.drop(columns=['residuals'])\n",
        "# max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 100\n",
        "\n",
        "# Step 3: Fit Random Forest on training residuals\n",
        "rf = RandomForestRegressor(n_estimators=30, random_state=42)\n",
        "rf.fit(train_feat_X, res_target_train)\n",
        "\n",
        "# Step 4: Predict residuals and add to TAR prediction\n",
        "res_pred_train = rf.predict(train_feat_X)\n",
        "res_pred_val = rf.predict(val_feat_X)\n",
        "res_pred_test = rf.predict(test_feat_X)\n",
        "\n",
        "# Align predicted residuals with TAR predictions\n",
        "yhat_train_corr = yhat_train[-len(res_pred_train):] + res_pred_train\n",
        "yhat_val_corr = yhat_val[-len(res_pred_val):] + res_pred_val\n",
        "yhat_test_corr = yhat_test[-len(res_pred_test):] + res_pred_test\n",
        "\n",
        "# True values (aligned)\n",
        "y_train_corr = y_train[-len(res_pred_train):]\n",
        "y_val_corr = y_val[-len(res_pred_val):]\n",
        "y_test_corr = y_test[-len(res_pred_test):]\n",
        "\n",
        "# Step 5: Evaluate corrected predictions\n",
        "print(\"\\n Final Evaluation after Residual Correction with RF:\")\n",
        "print(\"Train:\", evaluate(y_train_corr, yhat_train_corr))\n",
        "print(\"Val:  \", evaluate(y_val_corr, yhat_val_corr))\n",
        "print(\"Test: \", evaluate(y_test_corr, yhat_test_corr))\n",
        "\n",
        "# Plot corrected vs actual\n",
        "plot_actual_vs_pred(y_test_corr, yhat_test_corr, \"Test: Actual vs Corrected TAR+RF Forecast\")\n",
        "\n",
        "# ACF/PACF of corrected residuals\n",
        "final_residuals = y_test_corr - yhat_test_corr\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_acf(final_residuals, lags=30)\n",
        "plt.title(\"ACF of Final Residuals (TAR + RF)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_pacf(final_residuals, lags=30)\n",
        "plt.title(\"PACF of Final Residuals (TAR + RF)\")\n",
        "plt.show()\n",
        "\n",
        "# Final Evaluation Metrics for Corrected Forecast\n",
        "\n",
        "print(\"\\n Final Evaluation Metrics (TAR + RF Residual Correction):\")\n",
        "metrics_train = evaluate(y_train_corr, yhat_train_corr)\n",
        "metrics_val = evaluate(y_val_corr, yhat_val_corr)\n",
        "metrics_test = evaluate(y_test_corr, yhat_test_corr)\n",
        "\n",
        "# Print all metrics cleanly\n",
        "def print_metrics(metrics, label):\n",
        "    print(f\"\\n {label} Set Metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "print_metrics(metrics_train, \"Train\")\n",
        "print_metrics(metrics_val, \"Validation\")\n",
        "print_metrics(metrics_test, \"Test\")"
      ],
      "metadata": {
        "id": "6OFd3__lolGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PM10\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Load and prepare data\n",
        "df = pd.read_csv('/content/drive/MyDrive/CPCB.csv', parse_dates=['Datetime'])\n",
        "df.set_index('Datetime', inplace=True)\n",
        "\n",
        "features = ['PM2.5', 'SO2', 'CO', 'PM10', 'Temp', 'RH']\n",
        "df['PM10_roll3'] = df['PM10'].rolling(9).mean()\n",
        "df['Temp_roll3'] = df['Temp'].rolling(9).mean()\n",
        "df = df.dropna()\n",
        "\n",
        "df = df[features].dropna()\n",
        "\n",
        "# Split data\n",
        "n = len(df)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.85)\n",
        "\n",
        "train = df.iloc[:train_end]\n",
        "val = df.iloc[train_end:val_end]\n",
        "test = df.iloc[val_end:]\n",
        "\n",
        "# Lag function\n",
        "def create_lagged_data(df, lags=1):\n",
        "    df_lagged = df.copy()\n",
        "    for col in df.columns:\n",
        "        for lag in range(1, lags + 1):\n",
        "            df_lagged[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
        "    return df_lagged.dropna()\n",
        "\n",
        "# TAR model with polynomial features\n",
        "def fit_tar_model_poly(df_lagged, threshold):\n",
        "    poly = PolynomialFeatures(degree=1, include_bias=False)\n",
        "\n",
        "    regime_low = df_lagged[df_lagged['PM10_lag1'] <= threshold]\n",
        "    regime_high = df_lagged[df_lagged['PM10_lag1'] > threshold]\n",
        "\n",
        "    X_low = regime_low.drop(columns='PM10')\n",
        "    y_low = regime_low['PM10']\n",
        "    X_high = regime_high.drop(columns='PM10')\n",
        "    y_high = regime_high['PM10']\n",
        "\n",
        "    X_low_poly = poly.fit_transform(X_low)\n",
        "    X_high_poly = poly.fit_transform(X_high)\n",
        "\n",
        "    model_low = Ridge(alpha=0.01).fit(X_low_poly, y_low)\n",
        "    model_high = Ridge(alpha=0.01).fit(X_high_poly, y_high)\n",
        "\n",
        "\n",
        "    return model_low, model_high, threshold, poly\n",
        "\n",
        "# Prediction function\n",
        "def predict_tar_poly(df_lagged, model_low, model_high, threshold, poly):\n",
        "    X = df_lagged.drop(columns='PM10')\n",
        "    X_poly = poly.transform(X)\n",
        "    condition = df_lagged['PM10_lag1'] <= threshold\n",
        "    preds = np.where(condition,\n",
        "                     model_low.predict(X_poly),\n",
        "                     model_high.predict(X_poly))\n",
        "    return preds\n",
        "\n",
        "# Grid search over threshold\n",
        "df_lagged = create_lagged_data(df, lags=1)\n",
        "\n",
        "train_lagged = df_lagged.loc[train.index.intersection(df_lagged.index)]\n",
        "val_lagged = df_lagged.loc[val.index.intersection(df_lagged.index)]\n",
        "test_lagged = df_lagged.loc[test.index.intersection(df_lagged.index)]\n",
        "\n",
        "# Manually set best threshold and fit model\n",
        "best_thresh = 16.94\n",
        "model_low, model_high, _, poly = fit_tar_model_poly(train_lagged, best_thresh)\n",
        "best_models = (model_low, model_high, poly)\n",
        "\n",
        "# Optional: evaluate val RÂ² again\n",
        "yhat_val = predict_tar_poly(val_lagged, model_low, model_high, best_thresh, poly)\n",
        "best_r2 = r2_score(val_lagged['PM10'], yhat_val)\n",
        "print(f\"\\n Best threshold used: {best_thresh:.2f} with validation RÂ²: {best_r2:.4f}\")\n",
        "\n",
        "\n",
        "# Final model\n",
        "model_low, model_high, poly = best_models\n",
        "\n",
        "# Final predictions\n",
        "y_train = train_lagged['PM10']\n",
        "y_val = val_lagged['PM10']\n",
        "y_test = test_lagged['PM10']\n",
        "\n",
        "yhat_train = predict_tar_poly(train_lagged, model_low, model_high, best_thresh, poly)\n",
        "yhat_val = predict_tar_poly(val_lagged, model_low, model_high, best_thresh, poly)\n",
        "yhat_test = predict_tar_poly(test_lagged, model_low, model_high, best_thresh, poly)\n",
        "\n",
        "# Metrics\n",
        "def mase(y_true, y_pred, naive_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred)) / np.mean(np.abs(y_true[1:] - naive_pred[:-1]))\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    naive = y_true.shift(1).dropna()\n",
        "    y_true = y_true.iloc[1:]\n",
        "    y_pred = y_pred[1:]\n",
        "    return {\n",
        "        'R2': r2_score(y_true, y_pred),\n",
        "        'MAE': mean_absolute_error(y_true, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'MASE': mase(y_true, y_pred, naive)\n",
        "    }\n",
        "\n",
        "print(\"\\n Final Evaluation with Polynomial TAR:\")\n",
        "print(\"Train:\", evaluate(y_train, yhat_train))\n",
        "print(\"Val:  \", evaluate(y_val, yhat_val))\n",
        "print(\"Test: \", evaluate(y_test, yhat_test))\n",
        "\n",
        "# Plots\n",
        "def plot_actual_vs_pred(y_true, y_pred, title):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(y_true.index, y_true, label='Actual')\n",
        "    plt.plot(y_true.index, y_pred, label='Predicted')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('PM10')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_actual_vs_pred(y_train, yhat_train, 'Train: Actual vs Predicted')\n",
        "plot_actual_vs_pred(y_val, yhat_val, 'Validation: Actual vs Predicted')\n",
        "plot_actual_vs_pred(y_test, yhat_test, 'Test: Actual vs Predicted')\n",
        "\n",
        "# Residuals ACF and PACF\n",
        "residuals = y_test - yhat_test\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_acf(residuals, lags=30)\n",
        "plt.title(\"ACF of Test Residuals\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_pacf(residuals, lags=30)\n",
        "plt.title(\"PACF of Test Residuals\")\n",
        "plt.show()\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Create lagged features for residual correction (train only on past data)\n",
        "def create_residual_features(df_lagged, residuals, lags=1):\n",
        "    df_feat = df_lagged.copy()\n",
        "    df_feat['residuals'] = residuals\n",
        "    for lag in range(1, lags + 1):\n",
        "        df_feat[f'residuals_lag{lag}'] = df_feat['residuals'].shift(lag)\n",
        "    df_feat = df_feat.dropna()\n",
        "    return df_feat\n",
        "\n",
        "# Step 1: Compute residuals for train/val/test\n",
        "residuals_train = y_train - yhat_train\n",
        "residuals_val = y_val - yhat_val\n",
        "residuals_test = y_test - yhat_test\n",
        "\n",
        "# Step 2: Add residuals to original lagged features\n",
        "train_feat = create_residual_features(train_lagged.copy(), residuals_train, lags=1)\n",
        "val_feat = create_residual_features(val_lagged.copy(), residuals_val, lags=1)\n",
        "test_feat = create_residual_features(test_lagged.copy(), residuals_test, lags=1)\n",
        "\n",
        "# Ensure alignment\n",
        "res_target_train = train_feat['residuals']\n",
        "res_target_val = val_feat['residuals']\n",
        "res_target_test = test_feat['residuals']\n",
        "\n",
        "# Drop target from features\n",
        "train_feat_X = train_feat.drop(columns=['residuals'])\n",
        "val_feat_X = val_feat.drop(columns=['residuals'])\n",
        "test_feat_X = test_feat.drop(columns=['residuals'])\n",
        "# max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 100\n",
        "\n",
        "# Step 3: Fit Random Forest on training residuals\n",
        "rf = RandomForestRegressor(n_estimators=30, random_state=42)\n",
        "rf.fit(train_feat_X, res_target_train)\n",
        "\n",
        "# Step 4: Predict residuals and add to TAR prediction\n",
        "res_pred_train = rf.predict(train_feat_X)\n",
        "res_pred_val = rf.predict(val_feat_X)\n",
        "res_pred_test = rf.predict(test_feat_X)\n",
        "\n",
        "# Align predicted residuals with TAR predictions\n",
        "yhat_train_corr = yhat_train[-len(res_pred_train):] + res_pred_train\n",
        "yhat_val_corr = yhat_val[-len(res_pred_val):] + res_pred_val\n",
        "yhat_test_corr = yhat_test[-len(res_pred_test):] + res_pred_test\n",
        "\n",
        "# True values (aligned)\n",
        "y_train_corr = y_train[-len(res_pred_train):]\n",
        "y_val_corr = y_val[-len(res_pred_val):]\n",
        "y_test_corr = y_test[-len(res_pred_test):]\n",
        "\n",
        "# Step 5: Evaluate corrected predictions\n",
        "print(\"\\n Final Evaluation after Residual Correction with RF:\")\n",
        "print(\"Train:\", evaluate(y_train_corr, yhat_train_corr))\n",
        "print(\"Val:  \", evaluate(y_val_corr, yhat_val_corr))\n",
        "print(\"Test: \", evaluate(y_test_corr, yhat_test_corr))\n",
        "\n",
        "# Plot corrected vs actual\n",
        "plot_actual_vs_pred(y_test_corr, yhat_test_corr, \"Test: Actual vs Corrected TAR+RF Forecast\")\n",
        "\n",
        "# ACF/PACF of corrected residuals\n",
        "final_residuals = y_test_corr - yhat_test_corr\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_acf(final_residuals, lags=30)\n",
        "plt.title(\"ACF of Final Residuals (TAR + RF)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_pacf(final_residuals, lags=30)\n",
        "plt.title(\"PACF of Final Residuals (TAR + RF)\")\n",
        "plt.show()\n",
        "\n",
        "# Final Evaluation Metrics for Corrected Forecast\n",
        "\n",
        "print(\"\\n Final Evaluation Metrics (TAR + RF Residual Correction):\")\n",
        "metrics_train = evaluate(y_train_corr, yhat_train_corr)\n",
        "metrics_val = evaluate(y_val_corr, yhat_val_corr)\n",
        "metrics_test = evaluate(y_test_corr, yhat_test_corr)\n",
        "\n",
        "# Print all metrics cleanly\n",
        "def print_metrics(metrics, label):\n",
        "    print(f\"\\n {label} Set Metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "print_metrics(metrics_train, \"Train\")\n",
        "print_metrics(metrics_val, \"Validation\")\n",
        "print_metrics(metrics_test, \"Test\")"
      ],
      "metadata": {
        "id": "NyAqUa7QnuUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Load and prepare data\n",
        "df = pd.read_csv('/content/drive/MyDrive/CPCB.csv', parse_dates=['Datetime'])\n",
        "df.set_index('Datetime', inplace=True)\n",
        "\n",
        "features = ['PM2.5', 'SO2', 'CO', 'PM10', 'Temp', 'RH']\n",
        "df['CO'] = df['CO'].rolling(window=3).mean()\n",
        "df['_roll3'] = df['CO'].rolling(90).mean()\n",
        "df['Temp_roll3'] = df['Temp'].rolling(90).mean()\n",
        "df = df.dropna()\n",
        "\n",
        "df = df[features].dropna()\n",
        "\n",
        "# Split data\n",
        "n = len(df)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.85)\n",
        "\n",
        "train = df.iloc[:train_end]\n",
        "val = df.iloc[train_end:val_end]\n",
        "test = df.iloc[val_end:]\n",
        "\n",
        "# Lag function\n",
        "def create_lagged_data(df, lags=3):\n",
        "    df_lagged = df.copy()\n",
        "    for col in df.columns:\n",
        "        for lag in range(1, lags + 1):\n",
        "            df_lagged[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
        "    return df_lagged.dropna()\n",
        "\n",
        "# TAR model with polynomial features\n",
        "def fit_tar_model_poly(df_lagged, threshold):\n",
        "    poly = PolynomialFeatures(degree=1, include_bias=False)\n",
        "\n",
        "    regime_low = df_lagged[df_lagged['CO_lag1'] <= threshold]\n",
        "    regime_high = df_lagged[df_lagged['CO_lag1'] > threshold]\n",
        "\n",
        "    X_low = regime_low.drop(columns='CO')\n",
        "    y_low = regime_low['CO']\n",
        "    X_high = regime_high.drop(columns='CO')\n",
        "    y_high = regime_high['CO']\n",
        "\n",
        "    X_low_poly = poly.fit_transform(X_low)\n",
        "    X_high_poly = poly.fit_transform(X_high)\n",
        "\n",
        "    model_low = Ridge(alpha=1.0).fit(X_low_poly, y_low)  # Was 40.0\n",
        "    model_high = Ridge(alpha=1.0).fit(X_high_poly, y_high)\n",
        "\n",
        "\n",
        "    return model_low, model_high, threshold, poly\n",
        "\n",
        "# Prediction function\n",
        "def predict_tar_poly(df_lagged, model_low, model_high, threshold, poly):\n",
        "    X = df_lagged.drop(columns='CO')\n",
        "    X_poly = poly.transform(X)\n",
        "    condition = df_lagged['CO_lag1'] <= threshold\n",
        "    preds = np.where(condition,\n",
        "                     model_low.predict(X_poly),\n",
        "                     model_high.predict(X_poly))\n",
        "    return preds\n",
        "\n",
        "# Grid search over threshold\n",
        "df_lagged = create_lagged_data(df, lags=3)\n",
        "\n",
        "train_lagged = df_lagged.loc[train.index.intersection(df_lagged.index)]\n",
        "val_lagged = df_lagged.loc[val.index.intersection(df_lagged.index)]\n",
        "test_lagged = df_lagged.loc[test.index.intersection(df_lagged.index)]\n",
        "\n",
        "# Manually set best threshold and fit model\n",
        "best_thresh = 150.06\n",
        "model_low, model_high, _, poly = fit_tar_model_poly(train_lagged, best_thresh)\n",
        "best_models = (model_low, model_high, poly)\n",
        "\n",
        "# Optional: evaluate val RÂ² again\n",
        "yhat_val = predict_tar_poly(val_lagged, model_low, model_high, best_thresh, poly)\n",
        "best_r2 = r2_score(val_lagged['CO'], yhat_val)\n",
        "print(f\"\\n Best threshold used: {best_thresh:.2f} with validation RÂ²: {best_r2:.4f}\")\n",
        "\n",
        "\n",
        "# Final model\n",
        "model_low, model_high, poly = best_models\n",
        "\n",
        "# Final predictions\n",
        "y_train = train_lagged['CO']\n",
        "y_val = val_lagged['CO']\n",
        "y_test = test_lagged['CO']\n",
        "\n",
        "yhat_train = predict_tar_poly(train_lagged, model_low, model_high, best_thresh, poly)\n",
        "yhat_val = predict_tar_poly(val_lagged, model_low, model_high, best_thresh, poly)\n",
        "yhat_test = predict_tar_poly(test_lagged, model_low, model_high, best_thresh, poly)\n",
        "\n",
        "# Metrics\n",
        "def mase(y_true, y_pred, naive_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred)) / np.mean(np.abs(y_true[1:] - naive_pred[:-1]))\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    naive = y_true.shift(1).dropna()\n",
        "    y_true = y_true.iloc[1:]\n",
        "    y_pred = y_pred[1:]\n",
        "    return {\n",
        "        'R2': r2_score(y_true, y_pred),\n",
        "        'MAE': mean_absolute_error(y_true, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'MASE': mase(y_true, y_pred, naive)\n",
        "    }\n",
        "\n",
        "print(\"\\n Final Evaluation with Polynomial TAR:\")\n",
        "print(\"Train:\", evaluate(y_train, yhat_train))\n",
        "print(\"Val:  \", evaluate(y_val, yhat_val))\n",
        "print(\"Test: \", evaluate(y_test, yhat_test))\n",
        "\n",
        "# Plots\n",
        "def plot_actual_vs_pred(y_true, y_pred, title):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(y_true.index, y_true, label='Actual')\n",
        "    plt.plot(y_true.index, y_pred, label='Predicted')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('CO')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_actual_vs_pred(y_train, yhat_train, 'Train: Actual vs Predicted')\n",
        "plot_actual_vs_pred(y_val, yhat_val, 'Validation: Actual vs Predicted')\n",
        "plot_actual_vs_pred(y_test, yhat_test, 'Test: Actual vs Predicted')\n",
        "\n",
        "# Residuals ACF and PACF\n",
        "residuals = y_test - yhat_test\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_acf(residuals, lags=30)\n",
        "plt.title(\"ACF of Test Residuals\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_pacf(residuals, lags=30)\n",
        "plt.title(\"PACF of Test Residuals\")\n",
        "plt.show()\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create lagged features for residual correction (train only on past data)\n",
        "def create_residual_features(df_lagged, residuals, lags=3):\n",
        "    df_feat = df_lagged.copy()\n",
        "    df_feat['residuals'] = residuals\n",
        "    for lag in range(1, lags + 1):\n",
        "        df_feat[f'residuals_lag{lag}'] = df_feat['residuals'].shift(lag)\n",
        "    df_feat = df_feat.dropna()\n",
        "    return df_feat\n",
        "\n",
        "# Step 1: Compute residuals for train/val/test\n",
        "residuals_train = y_train - yhat_train\n",
        "residuals_val = y_val - yhat_val\n",
        "residuals_test = y_test - yhat_test\n",
        "\n",
        "# Step 2: Add residuals to original lagged features\n",
        "train_feat = create_residual_features(train_lagged.copy(), residuals_train, lags=3)\n",
        "val_feat = create_residual_features(val_lagged.copy(), residuals_val, lags=3)\n",
        "test_feat = create_residual_features(test_lagged.copy(), residuals_test, lags=3)\n",
        "\n",
        "# Ensure alignment\n",
        "res_target_train = train_feat['residuals']\n",
        "res_target_val = val_feat['residuals']\n",
        "res_target_test = test_feat['residuals']\n",
        "\n",
        "# Drop target from features\n",
        "train_feat_X = train_feat.drop(columns=['residuals'])\n",
        "val_feat_X = val_feat.drop(columns=['residuals'])\n",
        "test_feat_X = test_feat.drop(columns=['residuals'])\n",
        "# max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 100\n",
        "\n",
        "# Step 3: Fit Random Forest on training residuals\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(train_feat_X, res_target_train)\n",
        "\n",
        "# Step 4: Predict residuals and add to TAR prediction\n",
        "res_pred_train = rf.predict(train_feat_X)\n",
        "res_pred_val = rf.predict(val_feat_X)\n",
        "res_pred_test = rf.predict(test_feat_X)\n",
        "\n",
        "# Align predicted residuals with TAR predictions\n",
        "yhat_train_corr = yhat_train[-len(res_pred_train):] + res_pred_train\n",
        "yhat_val_corr = yhat_val[-len(res_pred_val):] + res_pred_val\n",
        "yhat_test_corr = yhat_test[-len(res_pred_test):] + res_pred_test\n",
        "\n",
        "# True values (aligned)\n",
        "y_train_corr = y_train[-len(res_pred_train):]\n",
        "y_val_corr = y_val[-len(res_pred_val):]\n",
        "y_test_corr = y_test[-len(res_pred_test):]\n",
        "\n",
        "# Step 5: Evaluate corrected predictions\n",
        "print(\"\\n Final Evaluation after Residual Correction with RF:\")\n",
        "print(\"Train:\", evaluate(y_train_corr, yhat_train_corr))\n",
        "print(\"Val:  \", evaluate(y_val_corr, yhat_val_corr))\n",
        "print(\"Test: \", evaluate(y_test_corr, yhat_test_corr))\n",
        "\n",
        "# Plot corrected vs actual\n",
        "plot_actual_vs_pred(y_test_corr, yhat_test_corr, \"Test: Actual vs Corrected TAR+RF Forecast\")\n",
        "\n",
        "# ACF/PACF of corrected residuals\n",
        "final_residuals = y_test_corr - yhat_test_corr\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_acf(final_residuals, lags=30)\n",
        "plt.title(\"ACF of Final Residuals (TAR + RF)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plot_pacf(final_residuals, lags=30)\n",
        "plt.title(\"PACF of Final Residuals (TAR + RF)\")\n",
        "plt.show()\n",
        "\n",
        "# Final Evaluation Metrics for Corrected Forecast\n",
        "\n",
        "print(\"\\n Final Evaluation Metrics (TAR + RF Residual Correction):\")\n",
        "metrics_train = evaluate(y_train_corr, yhat_train_corr)\n",
        "metrics_val = evaluate(y_val_corr, yhat_val_corr)\n",
        "metrics_test = evaluate(y_test_corr, yhat_test_corr)\n",
        "\n",
        "# Print all metrics cleanly\n",
        "def print_metrics(metrics, label):\n",
        "    print(f\"\\n {label} Set Metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "print_metrics(metrics_train, \"Train\")\n",
        "print_metrics(metrics_val, \"Validation\")\n",
        "print_metrics(metrics_test, \"Test\")\n"
      ],
      "metadata": {
        "id": "4xim-xksnHG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}